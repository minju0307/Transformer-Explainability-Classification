{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 불러오기 초기 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BERT_explainability.modules.BERT.ExplanationGenerator import Generator\n",
    "from BERT_explainability.modules.BERT.BertForSequenceClassification import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from captum.attr import visualization\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert로 model 불러오기 \n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('klue-bert-epoch-32').to(\"cuda:0\")\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode a sentence\n",
    "text_batch = [\"야 너희 립스틱 이쁘다 엄마가 성적 3등 올랐다고 사준거야 나 오늘 남자친구랑 데이트 하는데 좀 빌려줘라 안 돼 저번에도 너 빌려 가서 망가트렸잖아 너 많이 컸다 이제 말대꾸도 하냐 그게 아니라 엄마가 백화점 가서 특별히 사 준 건데 그러니까 더 좋네 남자친구한테 잘 보이고 오늘 내가 빌려 갈테니까 그런 줄 알아 어디 가서 꼬바르지 말고 알아서 잘 해라 안 되는데\"]\n",
    "encoding = tokenizer(text_batch, return_tensors='pt')\n",
    "input_ids = encoding['input_ids'].to(\"cuda:0\")\n",
    "attention_mask = encoding['attention_mask'].to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the explanations generator\n",
    "explanations = Generator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_LRP 코드 이해하기 (ExplanationGenerator.py 안)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one hot vector와 model.relprop의 cam을 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "output = model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "index = np.argmax(output.cpu().data.numpy(), axis=-1)\n",
    "one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
    "one_hot[0, index] = 1\n",
    "one_hot_vector = one_hot\n",
    "print(one_hot_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conservation:  tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor([[[ 6.6502e-06,  7.0124e-06,  6.5108e-06,  ...,  1.0495e-05,\n",
      "           1.2351e-05,  1.1076e-06],\n",
      "         [-6.0508e-08, -1.6266e-07,  1.4527e-05,  ...,  6.3141e-06,\n",
      "           2.7440e-06,  8.4078e-06],\n",
      "         [ 8.8628e-06,  1.2531e-05,  7.4550e-06,  ...,  1.0610e-05,\n",
      "           1.8788e-05,  2.2743e-05],\n",
      "         ...,\n",
      "         [-2.1188e-05,  4.8777e-05,  4.3548e-05,  ..., -3.2191e-05,\n",
      "           1.4737e-05, -3.9474e-05],\n",
      "         [ 8.6562e-07,  4.0281e-07,  3.0491e-06,  ...,  4.2468e-06,\n",
      "           8.0335e-06, -1.6084e-09],\n",
      "         [ 2.4430e-06, -3.4200e-08, -5.6509e-06,  ...,  2.0533e-07,\n",
      "          -2.6913e-06,  5.1070e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\"alpha\": 1}\n",
    "cam = model.relprop(torch.tensor(one_hot_vector).to(input_ids.device), **kwargs)\n",
    "print(cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 103, 768])\n"
     ]
    }
   ],
   "source": [
    "print(cam.shape) ## 103은 Input ids 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,  1396,  6928, 16960, 13854,  2062,  4122,  2116,  4610,    23,\n",
      "          2491,  4990,  4683,  1233,  2456,  2180,  2275,   717,  3822,  3997,\n",
      "          2720,  2251,  2379,  8698,  1889, 13964,  1556,  7004,  2810,  2181,\n",
      "          1378,   857, 14247,  6509,   743,  7004,   543,  2112, 13731,  2265,\n",
      "          2542,  9958,   743,  3732,  1718,  2062,  3699,  1041,  2104,  2660,\n",
      "          2119,  1889,  2529,  4603,  3614,  2181,  4122,  2116,  5269,   543,\n",
      "          2112,  7761,  1233,  1566,  6461,  4719,   831,  1560,  2203,  3997,\n",
      "          2720,  2251,  2470,  2201,  1521,  3783,  2088,  3822,   732,  2116,\n",
      "          7004,   547,  2201,  3707,  3637,  1567,  4860,  4069,   543,  2112,\n",
      "           676, 23746,  2118,  1041,  2088,  4860,  2112,  1521,  8982,  1378,\n",
      "           859, 13964,     3]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 103])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.relprop 상세"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (matmul1): MatMul()\n",
      "        (matmul2): MatMul()\n",
      "        (softmax): Softmax(dim=-1)\n",
      "        (add): Add()\n",
      "        (mul): Mul()\n",
      "        (clone): Clone()\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (add): Add()\n",
      "      )\n",
      "      (clone): Clone()\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELU()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (add): Add()\n",
      "    )\n",
      "    (clone): Clone()\n",
      "  )\n",
      "  (1): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (matmul1): MatMul()\n",
      "        (matmul2): MatMul()\n",
      "        (softmax): Softmax(dim=-1)\n",
      "        (add): Add()\n",
      "        (mul): Mul()\n",
      "        (clone): Clone()\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (add): Add()\n",
      "      )\n",
      "      (clone): Clone()\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELU()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (add): Add()\n",
      "    )\n",
      "    (clone): Clone()\n",
      "  )\n",
      "  (2): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (matmul1): MatMul()\n",
      "        (matmul2): MatMul()\n",
      "        (softmax): Softmax(dim=-1)\n",
      "        (add): Add()\n",
      "        (mul): Mul()\n",
      "        (clone): Clone()\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (add): Add()\n",
      "      )\n",
      "      (clone): Clone()\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELU()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (add): Add()\n",
      "    )\n",
      "    (clone): Clone()\n",
      "  )\n",
      "  (3): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (matmul1): MatMul()\n",
      "        (matmul2): MatMul()\n",
      "        (softmax): Softmax(dim=-1)\n",
      "        (add): Add()\n",
      "        (mul): Mul()\n",
      "        (clone): Clone()\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (add): Add()\n",
      "      )\n",
      "      (clone): Clone()\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELU()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (add): Add()\n",
      "    )\n",
      "    (clone): Clone()\n",
      "  )\n",
      "  (4): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (matmul1): MatMul()\n",
      "        (matmul2): MatMul()\n",
      "        (softmax): Softmax(dim=-1)\n",
      "        (add): Add()\n",
      "        (mul): Mul()\n",
      "        (clone): Clone()\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (add): Add()\n",
      "      )\n",
      "      (clone): Clone()\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELU()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (add): Add()\n",
      "    )\n",
      "    (clone): Clone()\n",
      "  )\n",
      "  (5): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (matmul1): MatMul()\n",
      "        (matmul2): MatMul()\n",
      "        (softmax): Softmax(dim=-1)\n",
      "        (add): Add()\n",
      "        (mul): Mul()\n",
      "        (clone): Clone()\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (add): Add()\n",
      "      )\n",
      "      (clone): Clone()\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELU()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (add): Add()\n",
      "    )\n",
      "    (clone): Clone()\n",
      "  )\n",
      "  (6): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (matmul1): MatMul()\n",
      "        (matmul2): MatMul()\n",
      "        (softmax): Softmax(dim=-1)\n",
      "        (add): Add()\n",
      "        (mul): Mul()\n",
      "        (clone): Clone()\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (add): Add()\n",
      "      )\n",
      "      (clone): Clone()\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELU()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (add): Add()\n",
      "    )\n",
      "    (clone): Clone()\n",
      "  )\n",
      "  (7): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (matmul1): MatMul()\n",
      "        (matmul2): MatMul()\n",
      "        (softmax): Softmax(dim=-1)\n",
      "        (add): Add()\n",
      "        (mul): Mul()\n",
      "        (clone): Clone()\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (add): Add()\n",
      "      )\n",
      "      (clone): Clone()\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELU()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (add): Add()\n",
      "    )\n",
      "    (clone): Clone()\n",
      "  )\n",
      "  (8): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (matmul1): MatMul()\n",
      "        (matmul2): MatMul()\n",
      "        (softmax): Softmax(dim=-1)\n",
      "        (add): Add()\n",
      "        (mul): Mul()\n",
      "        (clone): Clone()\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (add): Add()\n",
      "      )\n",
      "      (clone): Clone()\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELU()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (add): Add()\n",
      "    )\n",
      "    (clone): Clone()\n",
      "  )\n",
      "  (9): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (matmul1): MatMul()\n",
      "        (matmul2): MatMul()\n",
      "        (softmax): Softmax(dim=-1)\n",
      "        (add): Add()\n",
      "        (mul): Mul()\n",
      "        (clone): Clone()\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (add): Add()\n",
      "      )\n",
      "      (clone): Clone()\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELU()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (add): Add()\n",
      "    )\n",
      "    (clone): Clone()\n",
      "  )\n",
      "  (10): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (matmul1): MatMul()\n",
      "        (matmul2): MatMul()\n",
      "        (softmax): Softmax(dim=-1)\n",
      "        (add): Add()\n",
      "        (mul): Mul()\n",
      "        (clone): Clone()\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (add): Add()\n",
      "      )\n",
      "      (clone): Clone()\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELU()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (add): Add()\n",
      "    )\n",
      "    (clone): Clone()\n",
      "  )\n",
      "  (11): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (matmul1): MatMul()\n",
      "        (matmul2): MatMul()\n",
      "        (softmax): Softmax(dim=-1)\n",
      "        (add): Add()\n",
      "        (mul): Mul()\n",
      "        (clone): Clone()\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (add): Add()\n",
      "      )\n",
      "      (clone): Clone()\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELU()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (add): Add()\n",
      "    )\n",
      "    (clone): Clone()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.bert.encoder.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conservation:  tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.6502e-06,  7.0124e-06,  6.5108e-06,  ...,  1.0495e-05,\n",
       "           1.2351e-05,  1.1076e-06],\n",
       "         [-6.0508e-08, -1.6266e-07,  1.4527e-05,  ...,  6.3141e-06,\n",
       "           2.7440e-06,  8.4078e-06],\n",
       "         [ 8.8628e-06,  1.2531e-05,  7.4550e-06,  ...,  1.0610e-05,\n",
       "           1.8788e-05,  2.2743e-05],\n",
       "         ...,\n",
       "         [-2.1188e-05,  4.8777e-05,  4.3548e-05,  ..., -3.2191e-05,\n",
       "           1.4737e-05, -3.9474e-05],\n",
       "         [ 8.6562e-07,  4.0281e-07,  3.0491e-06,  ...,  4.2468e-06,\n",
       "           8.0335e-06, -1.6084e-09],\n",
       "         [ 2.4430e-06, -3.4200e-08, -5.6509e-06,  ...,  2.0533e-07,\n",
       "          -2.6913e-06,  5.1070e-06]]], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
    "one_hot = torch.sum(one_hot.cuda() * output)\n",
    "model.zero_grad()\n",
    "one_hot.backward(retain_graph=True)\n",
    "model.relprop(torch.tensor(one_hot_vector).to(input_ids.device), **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention gradients와 cam을 사용하고, rollout하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 103, 103])\n",
      "torch.Size([1, 103, 103])\n",
      "torch.Size([1, 103, 103])\n",
      "torch.Size([1, 103, 103])\n",
      "torch.Size([1, 103, 103])\n",
      "torch.Size([1, 103, 103])\n",
      "torch.Size([1, 103, 103])\n",
      "torch.Size([1, 103, 103])\n",
      "torch.Size([1, 103, 103])\n",
      "torch.Size([1, 103, 103])\n",
      "torch.Size([1, 103, 103])\n",
      "torch.Size([1, 103, 103])\n",
      "[tensor([[[1.5734e-10, 2.1191e-11, 2.0456e-11,  ..., 1.2488e-11,\n",
      "          3.1215e-11, 7.1514e-11],\n",
      "         [1.2349e-10, 3.1889e-10, 6.6744e-10,  ..., 1.0621e-11,\n",
      "          1.5431e-11, 1.3715e-11],\n",
      "         [2.0703e-11, 7.2510e-11, 1.0170e-10,  ..., 2.1040e-11,\n",
      "          1.6521e-10, 1.4448e-11],\n",
      "         ...,\n",
      "         [1.3246e-09, 5.4441e-09, 2.7564e-09,  ..., 7.1101e-10,\n",
      "          1.2557e-08, 1.8499e-08],\n",
      "         [4.1150e-12, 5.5394e-12, 2.9210e-11,  ..., 3.1085e-11,\n",
      "          1.0869e-11, 3.0344e-12],\n",
      "         [4.1889e-10, 1.6871e-10, 1.5483e-10,  ..., 5.0136e-10,\n",
      "          1.5928e-10, 5.3511e-10]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>), tensor([[[2.4835e-10, 1.0918e-10, 4.0924e-11,  ..., 4.5353e-11,\n",
      "          1.8764e-12, 5.2535e-10],\n",
      "         [2.0776e-12, 2.0084e-11, 7.2428e-11,  ..., 6.2560e-12,\n",
      "          3.2425e-12, 7.2483e-12],\n",
      "         [2.7777e-11, 2.2760e-10, 1.5027e-10,  ..., 4.0191e-12,\n",
      "          2.2631e-12, 2.7840e-11],\n",
      "         ...,\n",
      "         [1.1872e-12, 4.1253e-12, 1.0433e-11,  ..., 1.3136e-11,\n",
      "          1.4105e-11, 7.4798e-12],\n",
      "         [2.2456e-12, 7.6907e-13, 9.4251e-12,  ..., 7.5530e-12,\n",
      "          1.1726e-11, 4.2891e-12],\n",
      "         [6.2500e-11, 1.0749e-11, 1.4494e-11,  ..., 3.0129e-11,\n",
      "          4.8253e-11, 1.5211e-10]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>), tensor([[[2.1430e-09, 6.5909e-11, 2.0552e-10,  ..., 3.5113e-10,\n",
      "          2.0632e-10, 1.5563e-09],\n",
      "         [8.2767e-10, 1.0478e-10, 5.7435e-08,  ..., 3.2157e-11,\n",
      "          5.1298e-11, 5.8984e-10],\n",
      "         [6.0256e-11, 2.2876e-10, 1.7682e-10,  ..., 5.3943e-11,\n",
      "          5.5874e-11, 6.4375e-11],\n",
      "         ...,\n",
      "         [8.0618e-12, 4.8203e-11, 6.3672e-12,  ..., 6.5912e-10,\n",
      "          8.5917e-09, 1.4223e-10],\n",
      "         [4.0488e-11, 3.7651e-11, 1.2449e-10,  ..., 4.5688e-11,\n",
      "          2.8946e-11, 7.0516e-09],\n",
      "         [1.5828e-09, 4.7828e-10, 1.6386e-09,  ..., 2.1201e-09,\n",
      "          5.3219e-11, 5.5372e-08]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>), tensor([[[3.2004e-10, 1.3721e-11, 1.6833e-10,  ..., 1.2836e-11,\n",
      "          2.5145e-10, 2.4019e-10],\n",
      "         [1.4721e-11, 2.6871e-10, 1.0112e-11,  ..., 3.1594e-12,\n",
      "          6.6956e-12, 1.0340e-11],\n",
      "         [7.8322e-10, 1.3592e-09, 8.9516e-10,  ..., 7.1171e-12,\n",
      "          2.6089e-11, 4.1845e-10],\n",
      "         ...,\n",
      "         [6.3304e-11, 6.8486e-11, 4.2593e-12,  ..., 3.0550e-11,\n",
      "          6.8313e-10, 1.2841e-10],\n",
      "         [2.1731e-11, 6.9163e-10, 1.9112e-10,  ..., 1.5033e-11,\n",
      "          1.7035e-10, 1.2916e-10],\n",
      "         [4.1549e-09, 6.0553e-10, 1.3867e-09,  ..., 9.2355e-11,\n",
      "          2.7217e-09, 3.1440e-08]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>), tensor([[[7.5203e-11, 3.8194e-11, 6.6602e-11,  ..., 1.7869e-11,\n",
      "          9.2451e-11, 1.4266e-10],\n",
      "         [6.5289e-12, 3.2795e-12, 1.4693e-12,  ..., 1.3203e-13,\n",
      "          1.7749e-13, 1.2034e-11],\n",
      "         [1.8705e-12, 1.3280e-11, 4.6791e-14,  ..., 4.7851e-14,\n",
      "          2.4726e-14, 1.0683e-12],\n",
      "         ...,\n",
      "         [2.6511e-14, 1.0305e-13, 7.5763e-14,  ..., 3.4931e-13,\n",
      "          1.9605e-13, 1.3918e-13],\n",
      "         [5.1344e-14, 2.4239e-12, 8.9055e-13,  ..., 2.0767e-11,\n",
      "          9.3667e-13, 1.0449e-12],\n",
      "         [3.1126e-09, 1.6863e-09, 1.6818e-09,  ..., 7.5333e-10,\n",
      "          2.9049e-08, 5.9533e-09]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>), tensor([[[1.0222e-07, 8.3468e-09, 1.3809e-08,  ..., 1.1168e-09,\n",
      "          3.9726e-08, 0.0000e+00],\n",
      "         [2.8280e-09, 1.8191e-08, 5.5548e-09,  ..., 9.8356e-11,\n",
      "          5.5165e-10, 7.6113e-09],\n",
      "         [1.5796e-10, 3.3486e-11, 3.1872e-13,  ..., 2.7269e-11,\n",
      "          6.6115e-12, 1.3683e-10],\n",
      "         ...,\n",
      "         [2.6487e-10, 1.2956e-11, 7.5838e-11,  ..., 1.0690e-11,\n",
      "          1.0387e-09, 1.3994e-11],\n",
      "         [6.1732e-11, 5.9032e-11, 1.7055e-10,  ..., 7.5647e-11,\n",
      "          5.6545e-10, 8.3409e-10],\n",
      "         [1.8312e-07, 8.0133e-09, 3.3183e-09,  ..., 1.1842e-08,\n",
      "          1.1468e-08, 2.0281e-07]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>), tensor([[[3.0329e-08, 1.5705e-07, 2.3236e-07,  ..., 9.4657e-08,\n",
      "          5.1004e-08, 9.4301e-08],\n",
      "         [2.3495e-12, 2.0388e-08, 1.1118e-09,  ..., 3.2830e-12,\n",
      "          4.4965e-10, 4.5883e-10],\n",
      "         [4.7267e-11, 1.5923e-09, 5.1074e-11,  ..., 2.0670e-12,\n",
      "          2.9158e-11, 2.7454e-10],\n",
      "         ...,\n",
      "         [2.4421e-11, 1.0904e-12, 1.7150e-10,  ..., 3.3094e-10,\n",
      "          1.1277e-08, 1.1745e-11],\n",
      "         [1.8497e-10, 4.2641e-10, 9.6929e-14,  ..., 6.0709e-10,\n",
      "          4.8776e-11, 2.1807e-10],\n",
      "         [2.4359e-08, 3.7626e-10, 1.0567e-10,  ..., 9.3926e-11,\n",
      "          3.3729e-09, 2.0893e-08]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>), tensor([[[8.0255e-08, 6.0178e-08, 3.0900e-07,  ..., 5.7527e-08,\n",
      "          7.9681e-07, 9.1879e-08],\n",
      "         [3.5179e-09, 3.1874e-08, 4.2695e-09,  ..., 5.6965e-10,\n",
      "          2.9930e-10, 7.3608e-10],\n",
      "         [6.3235e-11, 1.3590e-10, 3.2128e-10,  ..., 2.9198e-12,\n",
      "          1.7187e-12, 3.4608e-12],\n",
      "         ...,\n",
      "         [7.2443e-10, 1.8561e-10, 8.3819e-10,  ..., 1.2788e-09,\n",
      "          4.7775e-09, 5.3480e-10],\n",
      "         [6.1515e-10, 1.5665e-10, 2.4555e-10,  ..., 3.3179e-10,\n",
      "          3.2309e-09, 5.4820e-10],\n",
      "         [1.8154e-08, 7.0436e-09, 8.9383e-09,  ..., 5.4465e-09,\n",
      "          1.0204e-08, 4.3952e-08]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>), tensor([[[1.3450e-07, 1.6511e-07, 1.8218e-08,  ..., 8.2053e-08,\n",
      "          1.6303e-07, 2.5866e-08],\n",
      "         [2.1279e-10, 2.6385e-10, 4.4483e-10,  ..., 1.5468e-10,\n",
      "          5.7316e-11, 4.7522e-10],\n",
      "         [2.1732e-11, 6.2992e-11, 5.7325e-11,  ..., 1.0324e-11,\n",
      "          2.6225e-12, 4.2174e-11],\n",
      "         ...,\n",
      "         [3.5989e-10, 1.0435e-10, 1.2148e-10,  ..., 5.7769e-09,\n",
      "          3.7137e-10, 5.1226e-11],\n",
      "         [1.1772e-09, 9.2972e-11, 4.8600e-12,  ..., 6.0211e-10,\n",
      "          2.4072e-10, 2.5277e-11],\n",
      "         [1.8275e-08, 1.1991e-08, 3.0469e-09,  ..., 9.1279e-09,\n",
      "          4.2708e-09, 2.3461e-08]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>), tensor([[[5.7542e-08, 1.1721e-06, 1.2227e-06,  ..., 7.4402e-08,\n",
      "          3.6533e-07, 4.2176e-08],\n",
      "         [1.7710e-11, 1.0449e-10, 7.1356e-11,  ..., 2.6071e-11,\n",
      "          2.0114e-11, 1.2924e-11],\n",
      "         [1.9269e-13, 9.2507e-13, 8.5885e-12,  ..., 5.9045e-14,\n",
      "          1.4535e-14, 4.3647e-14],\n",
      "         ...,\n",
      "         [5.7349e-10, 4.1244e-11, 1.9689e-11,  ..., 3.7807e-09,\n",
      "          5.0052e-09, 1.7742e-10],\n",
      "         [8.7581e-10, 8.1875e-11, 1.2549e-11,  ..., 4.2175e-09,\n",
      "          2.0379e-09, 1.0002e-10],\n",
      "         [9.8412e-10, 4.8664e-10, 3.1816e-09,  ..., 1.7435e-09,\n",
      "          1.0033e-09, 7.7503e-09]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>), tensor([[[5.1653e-07, 3.7561e-08, 3.1182e-11,  ..., 2.2093e-06,\n",
      "          8.2004e-07, 2.9821e-08],\n",
      "         [6.6862e-11, 6.8454e-11, 4.2804e-12,  ..., 2.0181e-11,\n",
      "          2.4125e-11, 2.7209e-12],\n",
      "         [1.8072e-15, 6.0382e-15, 7.6096e-14,  ..., 6.2945e-16,\n",
      "          5.0922e-16, 2.7077e-14],\n",
      "         ...,\n",
      "         [2.0111e-09, 1.6356e-11, 3.3117e-12,  ..., 3.7998e-09,\n",
      "          7.7585e-10, 5.2390e-12],\n",
      "         [2.2813e-09, 1.8208e-11, 3.3015e-12,  ..., 1.4347e-09,\n",
      "          2.8710e-09, 1.9472e-11],\n",
      "         [7.0880e-14, 1.9749e-13, 7.8342e-14,  ..., 9.5649e-14,\n",
      "          1.1294e-13, 2.5878e-13]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>), tensor([[[1.5024e-05, 3.7442e-06, 9.4348e-09,  ..., 1.0545e-05,\n",
      "          1.0326e-05, 4.7804e-10],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "blocks = model.bert.encoder.layer\n",
    "cams = []\n",
    "for blk in blocks:\n",
    "    grad = blk.attention.self.get_attn_gradients()\n",
    "    cam = blk.attention.self.get_attn_cam()\n",
    "    cam = cam[0].reshape(-1, cam.shape[-1], cam.shape[-1])\n",
    "    grad = grad[0].reshape(-1, grad.shape[-1], grad.shape[-1])\n",
    "    cam = grad * cam\n",
    "    cam = cam.clamp(min=0).mean(dim=0)\n",
    "    cams.append(cam.unsqueeze(0))\n",
    "    print(cam.unsqueeze(0).shape)\n",
    "\n",
    "print(cams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rollout_attention(all_layer_matrices, start_layer=0):\n",
    "    # adding residual consideration- code adapted from https://github.com/samiraabnar/attention_flow\n",
    "    num_tokens = all_layer_matrices[0].shape[1]\n",
    "    batch_size = all_layer_matrices[0].shape[0]\n",
    "    eye = torch.eye(num_tokens).expand(batch_size, num_tokens, num_tokens).to(all_layer_matrices[0].device)\n",
    "    all_layer_matrices = [all_layer_matrices[i] + eye for i in range(len(all_layer_matrices))]\n",
    "    matrices_aug = [all_layer_matrices[i] / all_layer_matrices[i].sum(dim=-1, keepdim=True)\n",
    "                          for i in range(len(all_layer_matrices))]\n",
    "    joint_attention = matrices_aug[start_layer]\n",
    "    for i in range(start_layer+1, len(matrices_aug)):\n",
    "        joint_attention = matrices_aug[i].bmm(joint_attention)\n",
    "    return joint_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[9.9954e-01, 5.3428e-06, 1.8052e-06,  ..., 1.3060e-05,\n",
      "          1.2558e-05, 2.8695e-07],\n",
      "         [7.6200e-09, 1.0000e+00, 6.9643e-08,  ..., 9.2456e-10,\n",
      "          1.4790e-09, 9.9302e-09],\n",
      "         [1.1842e-09, 3.7269e-09, 1.0000e+00,  ..., 1.2881e-10,\n",
      "          2.8959e-10, 9.8326e-10],\n",
      "         ...,\n",
      "         [5.3553e-09, 5.9267e-09, 4.0076e-09,  ..., 1.0000e+00,\n",
      "          4.5092e-08, 1.9572e-08],\n",
      "         [5.2646e-09, 1.5732e-09, 7.9207e-10,  ..., 7.3890e-09,\n",
      "          1.0000e+00, 8.9342e-09],\n",
      "         [2.5423e-07, 3.0861e-08, 2.3467e-08,  ..., 3.1751e-08,\n",
      "          6.2350e-08, 1.0000e+00]]], device='cuda:0', grad_fn=<BmmBackward0>)\n",
      "torch.Size([1, 103, 103])\n"
     ]
    }
   ],
   "source": [
    "rollout = compute_rollout_attention(cams, start_layer=0)\n",
    "print(rollout)\n",
    "print(rollout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5220e-08, device='cuda:0', grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(rollout[:, 0].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9995], device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(rollout[:, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9954e-01, 5.3428e-06, 1.8052e-06, 1.6658e-06, 6.0884e-07, 3.2363e-07,\n",
      "         1.3004e-06, 5.8906e-08, 1.8044e-07, 7.3464e-08, 9.0848e-08, 1.0320e-07,\n",
      "         2.3398e-07, 3.0671e-06, 1.1857e-06, 5.0555e-06, 8.3092e-06, 3.1887e-07,\n",
      "         2.0381e-07, 7.1675e-08, 1.9397e-07, 4.5220e-08, 2.0571e-07, 7.4925e-07,\n",
      "         8.1097e-07, 1.5228e-07, 2.8827e-05, 3.4590e-05, 1.5360e-05, 1.2209e-05,\n",
      "         1.4661e-05, 1.4500e-05, 2.5128e-06, 7.7164e-07, 6.0322e-06, 1.1441e-05,\n",
      "         3.0851e-06, 1.5179e-06, 7.2159e-06, 2.3456e-06, 6.2100e-07, 1.2983e-06,\n",
      "         2.2889e-07, 8.7674e-08, 2.4717e-06, 1.1569e-07, 3.0736e-07, 6.1155e-07,\n",
      "         3.4376e-07, 7.6198e-07, 7.2977e-08, 4.6611e-07, 1.2529e-06, 9.4217e-06,\n",
      "         8.7931e-06, 2.4748e-06, 2.4383e-06, 7.4513e-07, 1.5902e-06, 2.9190e-07,\n",
      "         2.1643e-07, 1.1251e-06, 8.0224e-06, 4.5463e-06, 7.4656e-06, 1.4571e-05,\n",
      "         6.2419e-06, 2.9069e-06, 8.6139e-06, 1.3326e-06, 4.0356e-06, 3.4298e-07,\n",
      "         9.4826e-08, 1.0640e-07, 1.7212e-06, 3.6732e-06, 9.1568e-07, 4.3445e-06,\n",
      "         7.3799e-06, 4.2185e-06, 2.2817e-05, 7.6824e-06, 9.2660e-06, 8.3999e-06,\n",
      "         7.6556e-06, 3.8689e-06, 3.2239e-06, 1.1142e-06, 8.8783e-07, 4.9684e-07,\n",
      "         2.3260e-06, 1.2465e-06, 8.7190e-07, 2.6078e-06, 1.0106e-06, 6.8405e-06,\n",
      "         6.3185e-06, 1.3829e-05, 1.7788e-05, 1.3614e-05, 1.3060e-05, 1.2558e-05,\n",
      "         2.8695e-07]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 103])\n"
     ]
    }
   ],
   "source": [
    "print(rollout[:, 0])\n",
    "print(rollout[:, 0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## expl 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conservation:  tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# encode a sentence\n",
    "text_batch = [\"야 너희 립스틱 이쁘다 엄마가 성적 3등 올랐다고 사준거야 나 오늘 남자친구랑 데이트 하는데 좀 빌려줘라 안 돼 저번에도 너 빌려 가서 망가트렸잖아 너 많이 컸다 이제 말대꾸도 하냐 그게 아니라 엄마가 백화점 가서 특별히 사 준 건데 그러니까 더 좋네 남자친구한테 잘 보이고 오늘 내가 빌려 갈테니까 그런 줄 알아 어디 가서 꼬바르지 말고 알아서 잘 해라 안 되는데\"]\n",
    "encoding = tokenizer(text_batch, return_tensors='pt')\n",
    "input_ids = encoding['input_ids'].to(\"cuda:0\")\n",
    "attention_mask = encoding['attention_mask'].to(\"cuda:0\")\n",
    "\n",
    "# initialize the explanations generator\n",
    "explanations = Generator(model)\n",
    "\n",
    "classifications = [\"020121\",\"000001\",\"02051\",\"020811\",\"020819\"]\n",
    "\n",
    "# generate an explanation for the input\n",
    "expl = explanations.generate_LRP(input_ids=input_ids, attention_mask=attention_mask, start_layer=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5220e-08, 5.3428e-06, 1.8052e-06, 1.6658e-06, 6.0884e-07, 3.2363e-07,\n",
      "        1.3004e-06, 5.8906e-08, 1.8044e-07, 7.3464e-08, 9.0848e-08, 1.0320e-07,\n",
      "        2.3398e-07, 3.0671e-06, 1.1857e-06, 5.0555e-06, 8.3092e-06, 3.1887e-07,\n",
      "        2.0381e-07, 7.1675e-08, 1.9397e-07, 4.5220e-08, 2.0571e-07, 7.4925e-07,\n",
      "        8.1097e-07, 1.5228e-07, 2.8827e-05, 3.4590e-05, 1.5360e-05, 1.2209e-05,\n",
      "        1.4661e-05, 1.4500e-05, 2.5128e-06, 7.7164e-07, 6.0322e-06, 1.1441e-05,\n",
      "        3.0851e-06, 1.5179e-06, 7.2159e-06, 2.3456e-06, 6.2100e-07, 1.2983e-06,\n",
      "        2.2889e-07, 8.7674e-08, 2.4717e-06, 1.1569e-07, 3.0736e-07, 6.1155e-07,\n",
      "        3.4376e-07, 7.6198e-07, 7.2977e-08, 4.6611e-07, 1.2529e-06, 9.4217e-06,\n",
      "        8.7931e-06, 2.4748e-06, 2.4383e-06, 7.4513e-07, 1.5902e-06, 2.9190e-07,\n",
      "        2.1643e-07, 1.1251e-06, 8.0224e-06, 4.5463e-06, 7.4656e-06, 1.4571e-05,\n",
      "        6.2419e-06, 2.9069e-06, 8.6139e-06, 1.3326e-06, 4.0356e-06, 3.4298e-07,\n",
      "        9.4826e-08, 1.0640e-07, 1.7212e-06, 3.6732e-06, 9.1568e-07, 4.3445e-06,\n",
      "        7.3799e-06, 4.2185e-06, 2.2817e-05, 7.6824e-06, 9.2660e-06, 8.3999e-06,\n",
      "        7.6556e-06, 3.8689e-06, 3.2239e-06, 1.1142e-06, 8.8783e-07, 4.9684e-07,\n",
      "        2.3260e-06, 1.2465e-06, 8.7190e-07, 2.6078e-06, 1.0106e-06, 6.8405e-06,\n",
      "        6.3185e-06, 1.3829e-05, 1.7788e-05, 1.3614e-05, 1.3060e-05, 1.2558e-05,\n",
      "        2.8695e-07], device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(expl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([103])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl = (expl - expl.min()) / (expl.max() - expl.min()) ## min-max scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 1.5335e-01, 5.0949e-02, 4.6912e-02, 1.6316e-02, 8.0594e-03,\n",
      "        3.6336e-02, 3.9620e-04, 3.9145e-03, 8.1762e-04, 1.3208e-03, 1.6785e-03,\n",
      "        5.4643e-03, 8.7476e-02, 3.3014e-02, 1.4504e-01, 2.3922e-01, 7.9217e-03,\n",
      "        4.5907e-03, 7.6582e-04, 4.3060e-03, 0.0000e+00, 4.6458e-03, 2.0380e-02,\n",
      "        2.2167e-02, 3.0993e-03, 8.3317e-01, 1.0000e+00, 4.4333e-01, 3.5211e-01,\n",
      "        4.2309e-01, 4.1843e-01, 7.1430e-02, 2.1028e-02, 1.7331e-01, 3.2988e-01,\n",
      "        8.7997e-02, 4.2632e-02, 2.0757e-01, 6.6590e-02, 1.6668e-02, 3.6273e-02,\n",
      "        5.3169e-03, 1.2290e-03, 7.0240e-02, 2.0399e-03, 7.5882e-03, 1.6394e-02,\n",
      "        8.6422e-03, 2.0749e-02, 8.0351e-04, 1.2184e-02, 3.4959e-02, 2.7143e-01,\n",
      "        2.5323e-01, 7.0332e-02, 6.9274e-02, 2.0261e-02, 4.4724e-02, 7.1407e-03,\n",
      "        4.9561e-03, 3.1260e-02, 2.3092e-01, 1.3030e-01, 2.1480e-01, 4.2049e-01,\n",
      "        1.7938e-01, 8.2840e-02, 2.4804e-01, 3.7266e-02, 1.1551e-01, 8.6196e-03,\n",
      "        1.4360e-03, 1.7709e-03, 4.8517e-02, 1.0502e-01, 2.5198e-02, 1.2445e-01,\n",
      "        2.1232e-01, 1.2081e-01, 6.5919e-01, 2.2108e-01, 2.6692e-01, 2.4185e-01,\n",
      "        2.2030e-01, 1.1069e-01, 9.2014e-02, 3.0946e-02, 2.4392e-02, 1.3073e-02,\n",
      "        6.6023e-02, 3.4773e-02, 2.3931e-02, 7.4182e-02, 2.7946e-02, 1.9671e-01,\n",
      "        1.8160e-01, 3.9901e-01, 5.1361e-01, 3.9280e-01, 3.7675e-01, 3.6221e-01,\n",
      "        6.9975e-03], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(expl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.219114140083548"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(expl.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "xai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
